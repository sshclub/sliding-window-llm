#!/usr/bin/env python3
"""
ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ëŒ€ìš©ëŸ‰ ë¡œê·¸ì™€ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ì„±ëŠ¥ ì¸¡ì •
"""

import os
import sys
import json
import time
import threading
import subprocess
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Tuple
import statistics
import concurrent.futures
from log_generator import LogGenerator

class PerformanceTester:
    def __init__(self):
        self.test_dir = "/home/ssh/work/sliding-window-llm"
        self.vllm_url = "http://127.0.0.1:8000/v1"
        self.model_name = "Qwen/Qwen2.5-7B-Instruct"
        self.results = []
    
    def check_vllm_server(self) -> bool:
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.vllm_url}/models", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def generate_large_logs(self) -> List[str]:
        """ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ë“¤ ìƒì„±"""
        print("ğŸ“ ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ ìƒì„± ì¤‘...")
        
        generator = LogGenerator()
        log_files = []
        
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ ë¡œê·¸ íŒŒì¼ ìƒì„±
        sizes = [
            (100, "small"),
            (500, "medium"), 
            (1000, "large"),
            (2000, "xlarge"),
            (5000, "huge")
        ]
        
        for size, name in sizes:
            print(f"  - {name} ë¡œê·¸ ìƒì„± ({size}ì¤„)...")
            logs = generator.generate_large_volume_logs(size)
            filename = f"perf_{name}_{size}lines.log"
            filepath = os.path.join(self.test_dir, filename)
            
            with open(filepath, 'w') as f:
                f.write('\n'.join(logs))
            
            log_files.append(filename)
        
        print(f"âœ… {len(log_files)}ê°œ ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ ìƒì„± ì™„ë£Œ")
        return log_files
    
    def measure_single_request_time(self, log_file: str, use_vllm: bool = True) -> Dict:
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •"""
        start_time = time.time()
        
        try:
            if use_vllm:
                # vLLM ì§ì ‘ í˜¸ì¶œë¡œ ì¸¡ì •
                with open(os.path.join(self.test_dir, log_file), 'r') as f:
                    log_content = f.read()
                
                # ê°„ë‹¨í•œ ë¶„ì„ ìš”ì²­
                payload = {
                    "model": self.model_name,
                    "messages": [
                        {"role": "system", "content": "You are a log analysis expert. Provide a brief analysis."},
                        {"role": "user", "content": f"Analyze these logs briefly:\n{log_content[:2000]}"}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 200
                }
                
                response = requests.post(
                    f"{self.vllm_url}/chat/completions",
                    json=payload,
                    timeout=60
                )
                
                if response.status_code == 200:
                    end_time = time.time()
                    return {
                        "success": True,
                        "duration": end_time - start_time,
                        "response_length": len(response.json()["choices"][0]["message"]["content"])
                    }
                else:
                    return {
                        "success": False,
                        "duration": time.time() - start_time,
                        "error": f"HTTP {response.status_code}"
                    }
            else:
                # ëª¨ì˜ í…ŒìŠ¤íŠ¸
                result = subprocess.run([
                    "python3", "test_pipeline.py"
                ], capture_output=True, text=True, cwd=self.test_dir, timeout=30)
                
                end_time = time.time()
                return {
                    "success": result.returncode == 0,
                    "duration": end_time - start_time,
                    "output": result.stdout
                }
                
        except Exception as e:
            return {
                "success": False,
                "duration": time.time() - start_time,
                "error": str(e)
            }
    
    def test_concurrent_requests(self, num_requests: int = 5) -> Dict:
        """ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ”„ ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ ({num_requests}ê°œ ìš”ì²­)...")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë¡œê·¸ ìƒì„±
        test_log = "2024-01-15 10:30:15 ERROR [test] Test error message\n" * 10
        test_file = "concurrent_test.log"
        with open(os.path.join(self.test_dir, test_file), 'w') as f:
            f.write(test_log)
        
        start_time = time.time()
        results = []
        
        def make_request(request_id: int) -> Dict:
            """ê°œë³„ ìš”ì²­ í•¨ìˆ˜"""
            req_start = time.time()
            try:
                payload = {
                    "model": self.model_name,
                    "messages": [
                        {"role": "system", "content": "You are a log analysis expert."},
                        {"role": "user", "content": f"Request {request_id}: Analyze this log briefly: {test_log[:500]}"}
                    ],
                    "temperature": 0.1,
                    "max_tokens": 100
                }
                
                response = requests.post(
                    f"{self.vllm_url}/chat/completions",
                    json=payload,
                    timeout=30
                )
                
                req_end = time.time()
                return {
                    "request_id": request_id,
                    "success": response.status_code == 200,
                    "duration": req_end - req_start,
                    "status_code": response.status_code
                }
            except Exception as e:
                return {
                    "request_id": request_id,
                    "success": False,
                    "duration": time.time() - req_start,
                    "error": str(e)
                }
        
        # ë™ì‹œ ìš”ì²­ ì‹¤í–‰
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_requests) as executor:
            futures = [executor.submit(make_request, i) for i in range(num_requests)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        total_time = time.time() - start_time
        successful_requests = sum(1 for r in results if r["success"])
        
        # í†µê³„ ê³„ì‚°
        durations = [r["duration"] for r in results if r["success"]]
        avg_duration = statistics.mean(durations) if durations else 0
        min_duration = min(durations) if durations else 0
        max_duration = max(durations) if durations else 0
        
        return {
            "total_requests": num_requests,
            "successful_requests": successful_requests,
            "total_time": total_time,
            "requests_per_second": num_requests / total_time,
            "avg_response_time": avg_duration,
            "min_response_time": min_duration,
            "max_response_time": max_duration,
            "results": results
        }
    
    def test_memory_usage(self, log_file: str) -> Dict:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ({log_file})...")
        
        try:
            # í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì „ ë©”ëª¨ë¦¬ í™•ì¸
            import psutil
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            start_time = time.time()
            result = subprocess.run([
                "python3", "log_llm_pipeline.py"
            ], capture_output=True, text=True, cwd=self.test_dir, timeout=120)
            
            end_time = time.time()
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            return {
                "success": result.returncode == 0,
                "duration": end_time - start_time,
                "initial_memory_mb": initial_memory,
                "final_memory_mb": final_memory,
                "memory_delta_mb": final_memory - initial_memory,
                "output": result.stdout
            }
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def test_token_processing_speed(self) -> Dict:
        """í† í° ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸"""
        print("âš¡ í† í° ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸...")
        
        # ë‹¤ì–‘í•œ í¬ê¸°ì˜ í…ìŠ¤íŠ¸ë¡œ í† í° ì²˜ë¦¬ ì†ë„ ì¸¡ì •
        test_texts = [
            "Short text",
            "Medium length text " * 50,
            "Long text " * 200,
            "Very long text " * 500
        ]
        
        results = []
        
        for i, text in enumerate(test_texts):
            start_time = time.time()
            
            try:
                import tiktoken
                enc = tiktoken.get_encoding("cl100k_base")
                tokens = enc.encode(text)
                token_count = len(tokens)
                
                end_time = time.time()
                duration = end_time - start_time
                
                results.append({
                    "text_size": len(text),
                    "token_count": token_count,
                    "duration": duration,
                    "tokens_per_second": token_count / duration if duration > 0 else 0
                })
            except Exception as e:
                results.append({
                    "text_size": len(text),
                    "error": str(e)
                })
        
        return {"token_processing_results": results}
    
    def run_performance_tests(self) -> None:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
        vllm_available = self.check_vllm_server()
        print(f"vLLM ì„œë²„ ìƒíƒœ: {'âœ… ì—°ê²°ë¨' if vllm_available else 'âŒ ì—°ê²° ì•ˆë¨'}")
        
        if not vllm_available:
            print("âš ï¸  vLLM ì„œë²„ê°€ ì—†ì–´ ì¼ë¶€ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 1. ëŒ€ìš©ëŸ‰ ë¡œê·¸ ìƒì„±
        large_logs = self.generate_large_logs()
        print()
        
        # 2. ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ í…ŒìŠ¤íŠ¸
        print("â±ï¸  ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬ ì‹œê°„ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        
        single_request_results = []
        for log_file in large_logs:
            print(f"  ğŸ“ {log_file} í…ŒìŠ¤íŠ¸ ì¤‘...")
            result = self.measure_single_request_time(log_file, use_vllm=vllm_available)
            result["log_file"] = log_file
            single_request_results.append(result)
            
            if result["success"]:
                print(f"    âœ… ì„±ê³µ: {result['duration']:.2f}ì´ˆ")
            else:
                print(f"    âŒ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        
        # 3. ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ (vLLM ì„œë²„ê°€ ìˆì„ ë•Œë§Œ)
        concurrent_results = None
        if vllm_available:
            print("\nğŸ”„ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
            print("-" * 40)
            concurrent_results = self.test_concurrent_requests(5)
            print(f"  ì´ ìš”ì²­: {concurrent_results['total_requests']}ê°œ")
            print(f"  ì„±ê³µ: {concurrent_results['successful_requests']}ê°œ")
            print(f"  ì´ ì‹œê°„: {concurrent_results['total_time']:.2f}ì´ˆ")
            print(f"  RPS: {concurrent_results['requests_per_second']:.2f}")
            print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {concurrent_results['avg_response_time']:.2f}ì´ˆ")
        
        # 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        print("\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        memory_result = self.test_memory_usage("test_log.txt")
        if memory_result["success"]:
            print(f"  ì²˜ë¦¬ ì‹œê°„: {memory_result['duration']:.2f}ì´ˆ")
            print(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_result['memory_delta_mb']:.1f}MB")
        else:
            print(f"  âŒ ì‹¤íŒ¨: {memory_result.get('error', 'Unknown error')}")
        
        # 5. í† í° ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸
        print("\nâš¡ í† í° ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸")
        print("-" * 40)
        token_results = self.test_token_processing_speed()
        for result in token_results["token_processing_results"]:
            if "error" not in result:
                print(f"  í…ìŠ¤íŠ¸ í¬ê¸°: {result['text_size']}ì, í† í°: {result['token_count']}ê°œ, ì†ë„: {result['tokens_per_second']:.0f}í† í°/ì´ˆ")
        
        # ê²°ê³¼ ì €ì¥
        self.save_performance_report({
            "timestamp": datetime.now().isoformat(),
            "vllm_available": vllm_available,
            "single_request_results": single_request_results,
            "concurrent_results": concurrent_results,
            "memory_result": memory_result,
            "token_results": token_results
        })
    
    def save_performance_report(self, data: Dict) -> None:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"performance_report_{timestamp}.json"
        report_path = os.path.join(self.test_dir, report_file)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")
        
        # ìš”ì•½ ì¶œë ¥
        print("\nğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìš”ì•½")
        print("=" * 40)
        
        if data["single_request_results"]:
            successful_results = [r for r in data["single_request_results"] if r["success"]]
            if successful_results:
                avg_duration = statistics.mean([r["duration"] for r in successful_results])
                print(f"í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_duration:.2f}ì´ˆ")
        
        if data["concurrent_results"]:
            print(f"ë™ì‹œ ìš”ì²­ ì²˜ë¦¬: {data['concurrent_results']['requests_per_second']:.2f} RPS")
        
        if data["memory_result"]["success"]:
            print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {data['memory_result']['memory_delta_mb']:.1f}MB")

def main():
    tester = PerformanceTester()
    
    print("ğŸ¯ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("1. ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("2. ë™ì‹œ ìš”ì²­ í…ŒìŠ¤íŠ¸ë§Œ")
    print("3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ë§Œ")
    print("4. í† í° ì²˜ë¦¬ ì†ë„ í…ŒìŠ¤íŠ¸ë§Œ")
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    if choice == "1":
        tester.run_performance_tests()
    elif choice == "2":
        if tester.check_vllm_server():
            result = tester.test_concurrent_requests(10)
            print(f"ë™ì‹œ ìš”ì²­ ê²°ê³¼: {result}")
        else:
            print("âŒ vLLM ì„œë²„ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    elif choice == "3":
        result = tester.test_memory_usage("test_log.txt")
        print(f"ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result}")
    elif choice == "4":
        result = tester.test_token_processing_speed()
        print(f"í† í° ì²˜ë¦¬ ê²°ê³¼: {result}")
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()
