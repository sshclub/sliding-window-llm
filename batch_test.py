#!/usr/bin/env python3
"""
ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ì—¬ëŸ¬ ë¡œê·¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import os
import sys
import json
import time
import subprocess
import requests
from datetime import datetime
from typing import List, Dict, Tuple
import glob

class BatchTester:
    def __init__(self):
        self.test_dir = "/home/ssh/work/sliding-window-llm"
        self.vllm_url = "http://127.0.0.1:8000/v1"
        self.model_name = "Qwen/Qwen2.5-7B-Instruct"
        self.results = []
    
    def check_vllm_server(self) -> bool:
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.vllm_url}/models", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def find_log_files(self, pattern: str = "*.log") -> List[str]:
        """ë¡œê·¸ íŒŒì¼ ì°¾ê¸°"""
        log_files = []
        for file in glob.glob(os.path.join(self.test_dir, pattern)):
            if os.path.isfile(file):
                log_files.append(os.path.basename(file))
        return sorted(log_files)
    
    def run_single_test(self, log_file: str, use_vllm: bool = True) -> Dict:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"  ğŸ“ {log_file} í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        start_time = time.time()
        script_name = "log_llm_pipeline.py" if use_vllm else "test_pipeline.py"
        script_path = os.path.join(self.test_dir, script_name)
        temp_script_path = os.path.join(self.test_dir, f"temp_{script_name}")
        
        try:
            # ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •
            with open(script_path, 'r') as f:
                content = f.read()
            
            # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ë³€ê²½
            if "test_log.txt" in content:
                modified_content = content.replace("./test_log.txt", f"./{log_file}")
            else:
                import re
                modified_content = re.sub(r'main\("\./[^"]+",', f'main("./{log_file}",', content)
            
            with open(temp_script_path, 'w') as f:
                f.write(modified_content)
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            result = subprocess.run([
                "python3", temp_script_path
            ], capture_output=True, text=True, cwd=self.test_dir, timeout=120)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.remove(temp_script_path)
            
            end_time = time.time()
            duration = end_time - start_time
            
            if result.returncode == 0:
                # ê²°ê³¼ íŒŒì¼ ì°¾ê¸°
                result_files = [f for f in os.listdir(self.test_dir) 
                              if f.startswith('analysis_results') or f.startswith('test_analysis_results')]
                latest_result = max(result_files, key=lambda x: os.path.getmtime(os.path.join(self.test_dir, x))) if result_files else None
                
                return {
                    "success": True,
                    "log_file": log_file,
                    "duration": duration,
                    "result_file": latest_result,
                    "output": result.stdout.strip()
                }
            else:
                return {
                    "success": False,
                    "log_file": log_file,
                    "duration": duration,
                    "error": result.stderr.strip()
                }
                
        except subprocess.TimeoutExpired:
            return {
                "success": False,
                "log_file": log_file,
                "duration": 120,
                "error": "Timeout (120ì´ˆ)"
            }
        except Exception as e:
            return {
                "success": False,
                "log_file": log_file,
                "duration": time.time() - start_time,
                "error": str(e)
            }
    
    def analyze_result_file(self, result_file: str) -> Dict:
        """ê²°ê³¼ íŒŒì¼ ë¶„ì„"""
        if not result_file:
            return {"windows": 0, "analysis_length": 0}
        
        filepath = os.path.join(self.test_dir, result_file)
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            total_analysis_length = sum(len(result.get('analysis', '')) for result in results)
            
            return {
                "windows": len(results),
                "analysis_length": total_analysis_length,
                "has_analysis": total_analysis_length > 0
            }
        except Exception as e:
            return {"windows": 0, "analysis_length": 0, "error": str(e)}
    
    def run_batch_test(self, test_vllm: bool = True, test_mock: bool = True) -> None:
        """ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
        
        # ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
        log_files = self.find_log_files()
        if not log_files:
            print("âŒ í…ŒìŠ¤íŠ¸í•  ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   ë¡œê·¸ë¥¼ ìƒì„±í•˜ë ¤ë©´: python3 log_generator.py")
            return
        
        print(f"ğŸ“ ë°œê²¬ëœ ë¡œê·¸ íŒŒì¼: {len(log_files)}ê°œ")
        for file in log_files:
            print(f"  - {file}")
        print()
        
        # vLLM ì„œë²„ ìƒíƒœ í™•ì¸
        vllm_available = self.check_vllm_server()
        print(f"vLLM ì„œë²„ ìƒíƒœ: {'âœ… ì—°ê²°ë¨' if vllm_available else 'âŒ ì—°ê²° ì•ˆë¨'}")
        
        if test_vllm and not vllm_available:
            print("âš ï¸  vLLM ì„œë²„ê°€ ì—†ì–´ vLLM í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            test_vllm = False
        
        total_tests = 0
        if test_vllm:
            total_tests += len(log_files)
        if test_mock:
            total_tests += len(log_files)
        
        print(f"ğŸ“Š ì´ {total_tests}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜ˆì •")
        print()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_count = 0
        
        # vLLM í…ŒìŠ¤íŠ¸
        if test_vllm:
            print("ğŸ”® vLLM í…ŒìŠ¤íŠ¸ ì‹œì‘")
            print("-" * 40)
            for log_file in log_files:
                test_count += 1
                print(f"[{test_count}/{total_tests}] vLLM - {log_file}")
                result = self.run_single_test(log_file, use_vllm=True)
                result["test_type"] = "vLLM"
                self.results.append(result)
                
                if result["success"]:
                    analysis_info = self.analyze_result_file(result["result_file"])
                    print(f"  âœ… ì„±ê³µ ({result['duration']:.1f}ì´ˆ, {analysis_info['windows']}ìœˆë„ìš°)")
                else:
                    print(f"  âŒ ì‹¤íŒ¨: {result['error']}")
                print()
        
        # ëª¨ì˜ í…ŒìŠ¤íŠ¸
        if test_mock:
            print("ğŸ§ª ëª¨ì˜ í…ŒìŠ¤íŠ¸ ì‹œì‘")
            print("-" * 40)
            for log_file in log_files:
                test_count += 1
                print(f"[{test_count}/{total_tests}] ëª¨ì˜ - {log_file}")
                result = self.run_single_test(log_file, use_vllm=False)
                result["test_type"] = "ëª¨ì˜"
                self.results.append(result)
                
                if result["success"]:
                    analysis_info = self.analyze_result_file(result["result_file"])
                    print(f"  âœ… ì„±ê³µ ({result['duration']:.1f}ì´ˆ, {analysis_info['windows']}ìœˆë„ìš°)")
                else:
                    print(f"  âŒ ì‹¤íŒ¨: {result['error']}")
                print()
    
    def generate_report(self) -> None:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        if not self.results:
            print("âŒ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë³´ê³ ì„œ")
        print("=" * 60)
        
        # ì „ì²´ í†µê³„
        total_tests = len(self.results)
        successful_tests = sum(1 for r in self.results if r["success"])
        failed_tests = total_tests - successful_tests
        
        print(f"ì „ì²´ í…ŒìŠ¤íŠ¸: {total_tests}ê°œ")
        print(f"ì„±ê³µ: {successful_tests}ê°œ ({successful_tests/total_tests*100:.1f}%)")
        print(f"ì‹¤íŒ¨: {failed_tests}ê°œ ({failed_tests/total_tests*100:.1f}%)")
        print()
        
        # í…ŒìŠ¤íŠ¸ íƒ€ì…ë³„ í†µê³„
        vllm_results = [r for r in self.results if r.get("test_type") == "vLLM"]
        mock_results = [r for r in self.results if r.get("test_type") == "ëª¨ì˜"]
        
        if vllm_results:
            vllm_success = sum(1 for r in vllm_results if r["success"])
            vllm_avg_time = sum(r["duration"] for r in vllm_results if r["success"]) / max(vllm_success, 1)
            print(f"vLLM í…ŒìŠ¤íŠ¸: {vllm_success}/{len(vllm_results)} ì„±ê³µ (í‰ê·  {vllm_avg_time:.1f}ì´ˆ)")
        
        if mock_results:
            mock_success = sum(1 for r in mock_results if r["success"])
            mock_avg_time = sum(r["duration"] for r in mock_results if r["success"]) / max(mock_success, 1)
            print(f"ëª¨ì˜ í…ŒìŠ¤íŠ¸: {mock_success}/{len(mock_results)} ì„±ê³µ (í‰ê·  {mock_avg_time:.1f}ì´ˆ)")
        print()
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ìƒì„¸
        failed_results = [r for r in self.results if not r["success"]]
        if failed_results:
            print("âŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for result in failed_results:
                print(f"  - {result['test_type']} - {result['log_file']}: {result['error']}")
            print()
        
        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ ìƒì„¸
        successful_results = [r for r in self.results if r["success"]]
        if successful_results:
            print("âœ… ì„±ê³µí•œ í…ŒìŠ¤íŠ¸:")
            for result in successful_results:
                analysis_info = self.analyze_result_file(result["result_file"])
                print(f"  - {result['test_type']} - {result['log_file']}: {result['duration']:.1f}ì´ˆ, {analysis_info['windows']}ìœˆë„ìš°")
        
        # ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"batch_test_report_{timestamp}.json"
        report_path = os.path.join(self.test_dir, report_file)
        
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "failed_tests": failed_tests,
                "success_rate": successful_tests/total_tests*100
            },
            "results": self.results
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ ì €ì¥: {report_file}")

def main():
    tester = BatchTester()
    
    print("ğŸ¯ ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ì˜µì…˜:")
    print("1. vLLM í…ŒìŠ¤íŠ¸ë§Œ")
    print("2. ëª¨ì˜ í…ŒìŠ¤íŠ¸ë§Œ")
    print("3. ë‘˜ ë‹¤ í…ŒìŠ¤íŠ¸")
    print("4. ìë™ (vLLM ì„œë²„ ìƒíƒœì— ë”°ë¼)")
    
    choice = input("\nì„ íƒ (1-4): ").strip()
    
    if choice == "1":
        tester.run_batch_test(test_vllm=True, test_mock=False)
    elif choice == "2":
        tester.run_batch_test(test_vllm=False, test_mock=True)
    elif choice == "3":
        tester.run_batch_test(test_vllm=True, test_mock=True)
    elif choice == "4":
        vllm_available = tester.check_vllm_server()
        tester.run_batch_test(test_vllm=vllm_available, test_mock=True)
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    tester.generate_report()

if __name__ == "__main__":
    main()
