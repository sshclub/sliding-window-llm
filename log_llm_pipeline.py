# vLLM Qwen 7B ì„œë²„ ì‹¤í–‰ ì˜ˆì‹œ
# python -m vllm.entrypoints.openai.api_server \
#   --model Qwen/Qwen2.5-7B-Instruct \
#   --max-model-len 16384 \
#   --tensor-parallel-size 1 \
#   --host 0.0.0.0 --port 8000

# log_llm_pipeline.py
import os, json, time
from datetime import datetime
from typing import List, Dict
import requests

# ìƒˆë¡œìš´ ëª¨ë“ˆ import
from prompt_templates import get_prompt_templates, AnalysisType
from sliding_window import create_sliding_window, WindowConfig, WindowProcessor
from config import OPENAI_BASE, MODEL, DEFAULT_WINDOW_TOKENS, DEFAULT_OVERLAP_RATIO, DEFAULT_MIN_TOKENS

# ìœˆë„ìš° ì„¤ì •
WINDOW_CONFIG = WindowConfig(
    max_tokens=DEFAULT_WINDOW_TOKENS,
    overlap_ratio=DEFAULT_OVERLAP_RATIO,
    min_tokens=DEFAULT_MIN_TOKENS
)

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ì€ ìƒˆë¡œìš´ ëª¨ë“ˆë¡œ ëŒ€ì²´ë¨

def call_llm(window_text: str, meta: Dict, analysis_type: AnalysisType = None) -> Dict:
    """LLM í˜¸ì¶œ í•¨ìˆ˜ - ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©"""
    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê°€ì ¸ì˜¤ê¸°
    prompt_templates = get_prompt_templates()
    
    # ë¶„ì„ íƒ€ì… ìë™ ê°ì§€ (ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš°)
    if analysis_type is None:
        analysis_type = prompt_templates.detect_analysis_type(window_text)
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ìƒì„±
    system_prompt = prompt_templates.get_system_prompt(analysis_type)
    user_prompt = prompt_templates.get_user_prompt(
        analysis_type,
        service=meta.get('service', '[unknown]'),
        host=meta.get('host', '[unknown]'),
        time_range=meta.get('time_range', '[unknown]'),
        severity=meta.get('severity', '[unknown]'),
        log_content=window_text
    )
    
    # ë¶„ì„ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    config = prompt_templates.get_analysis_config(analysis_type)
    
    payload = {
        "model": MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": config["temperature"],
        "max_tokens": config["max_tokens"],
    }
    
    r = requests.post(f"{OPENAI_BASE}/chat/completions", json=payload, timeout=config["timeout"])
    r.raise_for_status()
    data = r.json()
    content = data["choices"][0]["message"]["content"]
    
    return {
        "meta": meta, 
        "analysis": content,
        "analysis_type": analysis_type.value
    }

def main(log_path: str, out_path: str, meta: Dict, analysis_type: AnalysisType = None):
    """ë©”ì¸ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ - ìƒˆë¡œìš´ ëª¨ë“ˆ ì‚¬ìš©"""
    # ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
    sliding_window = create_sliding_window(WINDOW_CONFIG)
    windows = sliding_window.create_windows_from_file(log_path)
    
    if not windows:
        print("âŒ ìœˆë„ìš° ìƒì„± ì‹¤íŒ¨")
        return
    
    # ìœˆë„ìš° í†µê³„ ì¶œë ¥
    stats = sliding_window.get_window_stats(windows)
    print(f"ğŸ“Š ìœˆë„ìš° í†µê³„: {stats['total_windows']}ê°œ ìœˆë„ìš°, {stats['total_tokens']}ê°œ í† í°")
    
    results = []
    # ì‹œê°„ë²”ìœ„ ë©”íƒ€ ì¶”ì¶œ
    now = datetime.utcnow().isoformat() + "Z"
    meta = {**meta, "time_range": meta.get("time_range", f"processed_at={now}")}

    for window in windows:
        window_meta = {
            **meta, 
            "window_index": window.window_index, 
            "total_windows": window.total_windows,
            "window_tokens": window.token_count,
            "window_lines": window.end_line - window.start_line + 1
        }
        
        print(f"ğŸ” ìœˆë„ìš° {window.window_index + 1}/{window.total_windows} ë¶„ì„ ì¤‘... ({window.token_count}í† í°)")
        res = call_llm(window.content, window_meta, analysis_type)
        results.append(res)
        
        # ë°±í”„ë ˆì…” ë°©ì§€
        time.sleep(0.1)

    with open(out_path, "w") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {out_path} (ìœˆë„ìš°={len(windows)})")

if __name__ == "__main__":
    # ì‚¬ìš© ì˜ˆ
    meta = {"service": "ordersvc", "host": "node-01", "severity": "error>warning>info"}
    
    # ë¶„ì„ íƒ€ì… ì§€ì • (ì„ íƒì‚¬í•­)
    # analysis_type = AnalysisType.DATABASE  # ë°ì´í„°ë² ì´ìŠ¤ ì „ìš© ë¶„ì„
    # analysis_type = AnalysisType.MEMORY    # ë©”ëª¨ë¦¬ ì „ìš© ë¶„ì„
    analysis_type = None  # ìë™ ê°ì§€
    
    main("./scenario_1_ë°ì´í„°ë² ì´ìŠ¤_ì˜¤ë¥˜.log", "./analysis_results.json", meta, analysis_type)
