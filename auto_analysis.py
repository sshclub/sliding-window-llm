#!/usr/bin/env python3
"""
ìë™ ë¶„ì„ ì‹œìŠ¤í…œ - ì‹¤ì‹œê°„ ë¡œê·¸ë¥¼ ìë™ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥
"""

import os
import time
import json
import threading
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import re
from collections import defaultdict, deque
import requests

class AutoAnalyzer:
    def __init__(self, log_file: str = "realtime.log"):
        self.log_file = log_file
        self.running = False
        self.last_position = 0
        self.analysis_interval = 60  # 1ë¶„ë§ˆë‹¤ ë¶„ì„
        self.min_logs_for_analysis = 20  # ìµœì†Œ ë¡œê·¸ ìˆ˜
        self.max_logs_per_analysis = 100  # ë¶„ì„ë‹¹ ìµœëŒ€ ë¡œê·¸ ìˆ˜
        
        # vLLM ì„œë²„ ì„¤ì •
        self.vllm_url = "http://127.0.0.1:8000/v1"
        self.model_name = "Qwen/Qwen2.5-7B-Instruct"
        self.vllm_available = False
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_history = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ë¶„ì„ ê²°ê³¼
        self.analysis_count = 0
        
        # í†µê³„
        self.stats = {
            "total_analyses": 0,
            "successful_analyses": 0,
            "failed_analyses": 0,
            "total_logs_analyzed": 0,
            "start_time": datetime.now()
        }
        
        # ë¶„ì„ ì„ê³„ê°’
        self.thresholds = {
            "error_rate": 0.15,  # 15% ì´ìƒ ì—ëŸ¬
            "critical_count": 3,  # 3ê°œ ì´ìƒ í¬ë¦¬í‹°ì»¬
            "warning_count": 10,  # 10ê°œ ì´ìƒ ê²½ê³ 
            "service_errors": 5   # ì„œë¹„ìŠ¤ë‹¹ 5ê°œ ì´ìƒ ì—ëŸ¬
        }
    
    def check_vllm_server(self) -> bool:
        """vLLM ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            response = requests.get(f"{self.vllm_url}/models", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def parse_log_line(self, line: str) -> Optional[Dict]:
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (\w+) \[(\w+)\] (.+)'
        match = re.match(pattern, line.strip())
        
        if match:
            timestamp_str, level, service, message = match.groups()
            timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
            
            return {
                "timestamp": timestamp,
                "level": level,
                "service": service,
                "message": message,
                "raw": line.strip()
            }
        return None
    
    def read_new_logs(self) -> List[Dict]:
        """ìƒˆë¡œìš´ ë¡œê·¸ ì½ê¸°"""
        new_logs = []
        
        try:
            if not os.path.exists(self.log_file):
                return new_logs
            
            with open(self.log_file, 'r', encoding='utf-8', errors='ignore') as f:
                f.seek(self.last_position)
                lines = f.readlines()
                self.last_position = f.tell()
            
            for line in lines:
                parsed = self.parse_log_line(line)
                if parsed:
                    new_logs.append(parsed)
            
        except Exception as e:
            print(f"âŒ ë¡œê·¸ ì½ê¸° ì˜¤ë¥˜: {e}")
        
        return new_logs
    
    def should_analyze(self, logs: List[Dict]) -> bool:
        """ë¶„ì„ í•„ìš” ì—¬ë¶€ í™•ì¸"""
        if len(logs) < self.min_logs_for_analysis:
            return False
        
        # ì—ëŸ¬ìœ¨ ì²´í¬
        error_count = sum(1 for log in logs if log["level"] in ["ERROR", "CRITICAL"])
        error_rate = error_count / len(logs)
        if error_rate >= self.thresholds["error_rate"]:
            return True
        
        # í¬ë¦¬í‹°ì»¬ ë¡œê·¸ ì²´í¬
        critical_count = sum(1 for log in logs if log["level"] == "CRITICAL")
        if critical_count >= self.thresholds["critical_count"]:
            return True
        
        # ê²½ê³  ë¡œê·¸ ì²´í¬
        warning_count = sum(1 for log in logs if log["level"] == "WARN")
        if warning_count >= self.thresholds["warning_count"]:
            return True
        
        # ì„œë¹„ìŠ¤ë³„ ì—ëŸ¬ ì²´í¬
        service_errors = defaultdict(int)
        for log in logs:
            if log["level"] in ["ERROR", "CRITICAL"]:
                service_errors[log["service"]] += 1
        
        for service, count in service_errors.items():
            if count >= self.thresholds["service_errors"]:
                return True
        
        return False
    
    def analyze_with_vllm(self, logs: List[Dict]) -> Dict:
        """vLLMì„ ì‚¬ìš©í•œ ë¶„ì„"""
        try:
            # ë¡œê·¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            log_text = "\n".join([log["raw"] for log in logs])
            
            # ë¶„ì„ ìš”ì²­
            payload = {
                "model": self.model_name,
                "messages": [
                    {
                        "role": "system", 
                        "content": "You are an SRE/DevOps log expert. Analyze the logs and provide: (1) key symptoms, (2) root-cause hypotheses with priority, (3) concrete shell commands to verify, (4) mitigations/preventions, (5) confidence level. Keep answers concise but actionable."
                    },
                    {
                        "role": "user", 
                        "content": f"Analyze these logs:\n\n{log_text}"
                    }
                ],
                "temperature": 0.2,
                "max_tokens": 1200
            }
            
            response = requests.post(
                f"{self.vllm_url}/chat/completions",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                analysis = result["choices"][0]["message"]["content"]
                return {"success": True, "analysis": analysis}
            else:
                return {"success": False, "error": f"HTTP {response.status_code}"}
                
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def analyze_with_pipeline(self, logs: List[Dict]) -> Dict:
        """íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•œ ë¶„ì„"""
        try:
            # ì„ì‹œ ë¡œê·¸ íŒŒì¼ ìƒì„±
            temp_log_file = f"temp_auto_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
            with open(temp_log_file, 'w', encoding='utf-8') as f:
                for log in logs:
                    f.write(log["raw"] + "\n")
            
            # íŒŒì´í”„ë¼ì¸ ìŠ¤í¬ë¦½íŠ¸ ìˆ˜ì •
            with open("log_llm_pipeline.py", 'r') as f:
                content = f.read()
            
            modified_content = content.replace(
                'main("./scenario_1_ë°ì´í„°ë² ì´ìŠ¤_ì˜¤ë¥˜.log", "./analysis_results.json", meta)',
                f'main("./{temp_log_file}", "./temp_auto_analysis_results.json", meta)'
            )
            
            temp_script = f"temp_auto_pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.py"
            with open(temp_script, 'w') as f:
                f.write(modified_content)
            
            # ë¶„ì„ ì‹¤í–‰
            result = subprocess.run([
                "python3", temp_script
            ], capture_output=True, text=True, cwd=".", timeout=120)
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            os.remove(temp_script)
            os.remove(temp_log_file)
            
            if result.returncode == 0 and os.path.exists("temp_auto_analysis_results.json"):
                with open("temp_auto_analysis_results.json", 'r', encoding='utf-8') as f:
                    analysis_result = json.load(f)
                os.remove("temp_auto_analysis_results.json")
                
                return {
                    "success": True, 
                    "analysis": analysis_result[0]["analysis"] if analysis_result else "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
                }
            else:
                return {"success": False, "error": result.stderr}
                
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def run_analysis(self, logs: List[Dict]) -> Dict:
        """ë¶„ì„ ì‹¤í–‰"""
        print(f"ğŸ” ìë™ ë¶„ì„ ì‹¤í–‰ ì¤‘... ({len(logs)}ê°œ ë¡œê·¸)")
        
        # vLLM ì„œë²„ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if self.vllm_available:
            result = self.analyze_with_vllm(logs)
        else:
            result = self.analyze_with_pipeline(logs)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats["total_analyses"] += 1
        if result["success"]:
            self.stats["successful_analyses"] += 1
        else:
            self.stats["failed_analyses"] += 1
        
        self.stats["total_logs_analyzed"] += len(logs)
        
        return result
    
    def save_analysis_result(self, logs: List[Dict], analysis_result: Dict):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        self.analysis_count += 1
        
        result = {
            "analysis_id": self.analysis_count,
            "timestamp": datetime.now().isoformat(),
            "log_count": len(logs),
            "log_time_range": {
                "start": logs[0]["timestamp"].isoformat() if logs else None,
                "end": logs[-1]["timestamp"].isoformat() if logs else None
            },
            "log_summary": {
                "total": len(logs),
                "by_level": defaultdict(int),
                "by_service": defaultdict(int)
            },
            "analysis": analysis_result.get("analysis", "ë¶„ì„ ì‹¤íŒ¨"),
            "success": analysis_result["success"]
        }
        
        # ë¡œê·¸ ìš”ì•½ ìƒì„±
        for log in logs:
            result["log_summary"]["by_level"][log["level"]] += 1
            result["log_summary"]["by_service"][log["service"]] += 1
        
        # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.analysis_history.append(result)
        
        # íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        result_file = f"auto_analysis_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“„ ìë™ ë¶„ì„ ê²°ê³¼ ì €ì¥: {result_file}")
        
        # ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        if analysis_result["success"]:
            print("âœ… ë¶„ì„ ì™„ë£Œ")
            # ë¶„ì„ ê²°ê³¼ì˜ ì²« 200ìë§Œ ì¶œë ¥
            analysis_preview = analysis_result["analysis"][:200] + "..." if len(analysis_result["analysis"]) > 200 else analysis_result["analysis"]
            print(f"ğŸ“‹ ë¶„ì„ ìš”ì•½: {analysis_preview}")
        else:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {analysis_result['error']}")
    
    def print_status(self):
        """ìƒíƒœ ì¶œë ¥"""
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds()
        success_rate = (self.stats["successful_analyses"] / max(self.stats["total_analyses"], 1)) * 100
        
        print(f"\nğŸ“Š ìë™ ë¶„ì„ ìƒíƒœ")
        print(f"ì‹¤í–‰ ì‹œê°„: {elapsed:.0f}ì´ˆ")
        print(f"ì´ ë¶„ì„: {self.stats['total_analyses']}íšŒ")
        print(f"ì„±ê³µ: {self.stats['successful_analyses']}íšŒ ({success_rate:.1f}%)")
        print(f"ì‹¤íŒ¨: {self.stats['failed_analyses']}íšŒ")
        print(f"ë¶„ì„ëœ ë¡œê·¸: {self.stats['total_logs_analyzed']}ê°œ")
        print(f"vLLM ì„œë²„: {'âœ… ì—°ê²°ë¨' if self.vllm_available else 'âŒ ì—°ê²° ì•ˆë¨'}")
    
    def start_auto_analysis(self):
        """ìë™ ë¶„ì„ ì‹œì‘"""
        print(f"ğŸ¤– ìë™ ë¶„ì„ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"ëª¨ë‹ˆí„°ë§ íŒŒì¼: {self.log_file}")
        print(f"ë¶„ì„ ê°„ê²©: {self.analysis_interval}ì´ˆ")
        print(f"ìµœì†Œ ë¡œê·¸ ìˆ˜: {self.min_logs_for_analysis}ê°œ")
        print("ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
        print("-" * 50)
        
        self.running = True
        last_status_time = time.time()
        accumulated_logs = []
        
        try:
            while self.running:
                # vLLM ì„œë²„ ìƒíƒœ í™•ì¸ (1ë¶„ë§ˆë‹¤)
                if time.time() - last_status_time >= 60:
                    self.vllm_available = self.check_vllm_server()
                    last_status_time = time.time()
                
                # ìƒˆë¡œìš´ ë¡œê·¸ ì½ê¸°
                new_logs = self.read_new_logs()
                
                if new_logs:
                    accumulated_logs.extend(new_logs)
                    print(f"ğŸ“ ìƒˆ ë¡œê·¸ {len(new_logs)}ê°œ ê°ì§€ (ëˆ„ì : {len(accumulated_logs)}ê°œ)")
                    
                    # ë¶„ì„ í•„ìš” ì—¬ë¶€ í™•ì¸
                    if self.should_analyze(accumulated_logs):
                        # ë¶„ì„í•  ë¡œê·¸ ì„ íƒ (ìµœê·¼ ë¡œê·¸)
                        analysis_logs = accumulated_logs[-self.max_logs_per_analysis:]
                        
                        # ë¶„ì„ ì‹¤í–‰
                        analysis_result = self.run_analysis(analysis_logs)
                        
                        # ê²°ê³¼ ì €ì¥
                        self.save_analysis_result(analysis_logs, analysis_result)
                        
                        # ëˆ„ì  ë¡œê·¸ ì´ˆê¸°í™”
                        accumulated_logs = []
                
                # ìƒíƒœ ì¶œë ¥ (5ë¶„ë§ˆë‹¤)
                if time.time() - last_status_time >= 300:
                    self.print_status()
                    last_status_time = time.time()
                
                # 10ì´ˆ ëŒ€ê¸°
                time.sleep(10)
                
        except KeyboardInterrupt:
            print(f"\nğŸ›‘ ìë™ ë¶„ì„ ì¤‘ë‹¨")
        finally:
            self.running = False
            self.print_status()
            
            # ìµœì¢… í†µê³„ ì €ì¥
            stats_file = f"auto_analysis_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(stats_file, 'w', encoding='utf-8') as f:
                json.dump(self.stats, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ“„ ìë™ ë¶„ì„ í†µê³„ ì €ì¥: {stats_file}")

def main():
    print("ğŸ¤– ìë™ ë¶„ì„ ì‹œìŠ¤í…œ")
    print("=" * 50)
    
    # ì„¤ì • ì…ë ¥
    log_file = input("ëª¨ë‹ˆí„°ë§í•  ë¡œê·¸ íŒŒì¼ (ê¸°ë³¸: realtime.log): ").strip() or "realtime.log"
    
    try:
        analysis_interval = int(input("ë¶„ì„ ê°„ê²©(ì´ˆ) (ê¸°ë³¸: 60): ").strip() or "60")
    except ValueError:
        analysis_interval = 60
    
    try:
        min_logs = int(input("ìµœì†Œ ë¡œê·¸ ìˆ˜ (ê¸°ë³¸: 20): ").strip() or "20")
    except ValueError:
        min_logs = 20
    
    try:
        max_logs = int(input("ë¶„ì„ë‹¹ ìµœëŒ€ ë¡œê·¸ ìˆ˜ (ê¸°ë³¸: 100): ").strip() or "100")
    except ValueError:
        max_logs = 100
    
    # ë¶„ì„ê¸° ìƒì„± ë° ì‹œì‘
    analyzer = AutoAnalyzer(log_file)
    analyzer.analysis_interval = analysis_interval
    analyzer.min_logs_for_analysis = min_logs
    analyzer.max_logs_per_analysis = max_logs
    
    analyzer.start_auto_analysis()

if __name__ == "__main__":
    main()
